<div align="center">

<p align="center">
  <img src="./assets/logo.png" width="200px" alt="Live Avatar Teaser">
</p>

<h1>ğŸ¬ Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length</h1>

<p>
<a href="#" style="color: inherit;">Yubo Huang</a><sup>1,2</sup> Â·
<a href="#" style="color: inherit;">Hailong Guo</a><sup>1,3</sup> Â·
<a href="#" style="color: inherit;">Fangtai Wu</a><sup>1,4</sup> Â·
<a href="#" style="color: inherit;">Shifeng Zhang</a><sup>1</sup> Â·
<a href="#" style="color: inherit;">Shijie Huang</a><sup>1</sup> Â·
<a href="#" style="color: inherit;">Qijun Gan</a><sup>4</sup> Â·
<a href="#" style="color: inherit;">Lin Liu</a><sup>2</sup> Â·
<a href="#" style="color: inherit;">Sirui Zhao</a><sup>2,*</sup> Â·
<a href="#" style="color: inherit;">Enhong Chen</a><sup>2,*</sup> Â·
<a href="#" style="color: inherit;">Jiaming Liu</a><sup>1,â€¡</sup> Â·
<a href="#" style="color: inherit;">Steven Hoi</a><sup>1</sup>
</p>

<p style="font-size: 0.9em;">
<sup>1</sup> Alibaba Group &nbsp;&nbsp;
<sup>2</sup> University of Science and Technology of China &nbsp;&nbsp;
<sup>3</sup> Beijing University of Posts and Telecommunications &nbsp;&nbsp;
<sup>4</sup> Zhejiang University
</p>

<p style="font-size: 0.9em;">
<sup>*</sup> Corresponding authors. &nbsp;&nbsp; <sup>â€¡</sup> Project leader.
</p>

<!-- Badges -->
<a href="https://arxiv.org/abs/YOUR_PAPER_ID"><img src="https://img.shields.io/badge/arXiv-25XX.XXXXX-b31b1b.svg?style=for-the-badge" alt="arXiv"></a> <a href="https://huggingface.co/YOUR_ORG/YOUR_MODEL"><img src="https://img.shields.io/badge/Hugging%20Face-Model-ffbd45?style=for-the-badge&logo=huggingface&logoColor=white" alt="HuggingFace"></a> <a href="https://github.com/YOUR_USERNAME/LiveAvatar"><img src="https://img.shields.io/badge/Github-Code-black?style=for-the-badge&logo=github" alt="Github"></a> <a href="YOUR_PROJECT_PAGE_URL"><img src="https://img.shields.io/badge/Project-Page-blue?style=for-the-badge&logo=googlechrome&logoColor=white" alt="Project Page"></a>

</div>

> **TL;DR:** **Live Avatar** is an algorithmâ€“system co-designed framework that enables real-time, streaming, infinite-length interactive avatar video generation. Powered by a **14B-parameter** diffusion model, it achieves **20 FPS** on **5Ã—H800** GPUs with **4-step** sampling and supports **Block-wise Autoregressive** processing for **10,000+** second streaming videos.

<p align="center">
  <iframe
    width="800"
    height="450"
    src="https://www.youtube.com/embed/srbsGlLNpAc?autoplay=1&mute=1&loop=1&playlist=srbsGlLNpAc&controls=1&rel=0"
    title="Live Avatar Demo"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
    allowfullscreen>
  </iframe>
</p>

---
## âœ¨ Highlights

<!-- We propose **MultiTalk** , a novel framework for audio-driven multi-person conversational video generation. Given a multi-stream audio input, a reference image and a prompt, MultiTalk generates a video containing interactions following the prompt, with consistent lip motions aligned with the audio. -->

> - âš¡ **â€‹â€‹Real-time Streaming Interaction**â€‹â€‹ - Achieve 20 FPS real-time streaming with low latency
> - â™¾ï¸ â€‹â€‹**â€‹â€‹Infinite-length Autoregressive Generation**â€‹â€‹â€‹â€‹ - Support 10,000+ second continuous video generation
> - ğŸ¨ â€‹â€‹**â€‹â€‹Generalization Performances**â€‹â€‹â€‹â€‹ - Strong generalization across cartoon characters, singing, and diverse scenarios 


---
## ğŸ“° News
- **[2025.12.02]** The code will be open source in early December.
- **[2025.12.02]** We release Paper and demo page Website.
<!-- - **[2025/09]** Paper accepted to **CVPR/ICCV 2025**. -->

---

## ğŸ“‘ Todo List

### ğŸŒŸ **Early December** (core code release)

- âœ… Release the paper
- âœ… Release the demo website
- â¬œ Release inference code
- â¬œ Release checkpoints on Hugging Face
- â¬œ Release Gradio demo
- â¬œ Experimental real-time streaming inference on H800 GPUs.
  - â¬œ Distribution-matching distillation to 4 steps
  - â¬œ Timestep-forcing pipeline parallelism

### âš™ï¸ **Later updates**

- â¬œ Optimized real-time streaming inference on RTX 4090 / A100 GPUs.
  - â¬œ Distribution-matching distillation to 3 steps
  - â¬œ Timestep-forcing pipeline parallelism
  - â¬œ SVD quantization
  - â¬œ SageAttention integration
- â¬œ Run with very low VRAM
- â¬œ TTS integration
- â¬œ ComfyUI support
- â¬œ 1.3B model

<!-- ## ğŸ› ï¸ Installation

Please follow the steps below to set up the environment.

### 1. Create Environment
```bash
conda create -n liveavatar python=3.10 -y
conda activate liveavatar
```

### 2. Install CUDA Dependencies
```bash
conda install nvidia/label/cuda-12.4.1::cuda -y
conda install -c nvidia/label/cuda-12.4.1 cudatoolkit -y
```

### 3. Install PyTorch & Flash Attention
```bash
pip install torch==2.8.0 torchvision==0.23.0 --index-url https://download.pytorch.org/whl/cu128

pip install flash-attn==2.8.3 --no-build-isolation
```

### 4. Install Python Requirements
```bash
pip install -r requirements.txt
```

--- -->

<!-- ## ğŸ“¥ Download Models

Please download the pretrained checkpoints from [Hugging Face](https://huggingface.co/) and place them in the `checkpoints/` directory.

| Model Component | Description | Link |
| :--- | :--- | :---: |
| `live_avatar` | our model| [Download](#) |
```bash
mkdir -p checkpoints -->
<!-- # Move your downloaded files here -->
<!-- ``` -->

<!-- --- -->

<!-- ## ğŸš€ Inference

### Streaming Real-time Infinite Inference


```bash
# Recommended: Run with relative path
bash infinite_inference.sh
```

---


--- -->

## ğŸ“ Citation

If you find this project useful for your research, please consider citing our paper:

```bibtex
@article{placeholder
}
```

## ğŸ™ Acknowledgements

We would like to express our gratitude to the following projects:

*   [CausVid](https://github.com/tianweiy/CausVid)
*   [Longlive](https://github.com/NVlabs/LongLive)
*   [WanS2V](https://humanaigc.github.io/wan-s2v-webpage/)